# -*- coding: utf-8 -*-
"""Kayla Ahrndt- DSC 4900 Project/Portfolio Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11t9wQPjdzzpEXytxBFbOINsXDdJ-2C4C
"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd
import numpy as np

# Read in the data
shipment_pricing = '/content/gdrive/MyDrive/Supply_Chain_Shipment_Pricing_Dataset.csv'

shipment_pricing = pd.read_csv(shipment_pricing)

shipment_pricing.head()

# Feature names
shipment_pricing.columns

# Re-parse with the correct datetime format (m/d/y)
shipment_pricing['po sent to vendor date'] = pd.to_datetime(shipment_pricing['po sent to vendor date'], format='%m/%d/%Y')
shipment_pricing['delivered to client date'] = pd.to_datetime(shipment_pricing['delivered to client date'], format='mixed')

# Temporarily coerce to find bad rows
shipment_pricing['temp_sent'] = pd.to_datetime(shipment_pricing['po sent to vendor date'], errors='coerce')
shipment_pricing['temp_delivered'] = pd.to_datetime(shipment_pricing['delivered to client date'], errors='coerce')
# Remove invalid dates
shipment_pricing = shipment_pricing[shipment_pricing['po sent to vendor date'].notna() & shipment_pricing['delivered to client date'].notna()]

# Remove rows where parsing failed (bad strings)
shipment_pricing = shipment_pricing[shipment_pricing['temp_sent'].notna() & shipment_pricing['temp_delivered'].notna()]
# Drop temp columns
shipment_pricing.drop(columns=['temp_sent', 'temp_delivered'], inplace=True)

# Lead time column
df_filtered = shipment_pricing.copy()
df_filtered['lead_time'] = (df_filtered['delivered to client date'] - df_filtered['po sent to vendor date']).dt.days
# Remove rows with missing or negative lead times
df_filtered = df_filtered[df_filtered['lead_time'] > 0]

# Remove 'Truck' shipment mode (only one instance)
df_filtered = df_filtered[df_filtered['shipment mode'].str.lower() != 'truck']

# Remove specific unwanted text values from 'weight (kilograms)'
df_filtered = df_filtered[~df_filtered['weight (kilograms)'].astype(str).isin(['Freight Included in Commodity Cost'])]
df_filtered = df_filtered[~df_filtered['weight (kilograms)'].astype(str).str.startswith('See ASN')]

# Clean numeric columns
numeric_cols = ['freight cost (usd)', 'lead_time', 'weight (kilograms)']
# Make sure to work on a copy explicitly
df_filtered = df_filtered.copy()
for col in numeric_cols:
    # Keep only rows where the column is numeric (allowing decimals)
    df_filtered = df_filtered[df_filtered[col].apply(lambda x: str(x).replace('.', '', 1).isdigit())]
    # Safely convert to numeric
    df_filtered.loc[:, col] = pd.to_numeric(df_filtered[col], errors='coerce')
    # Log transform
    df_filtered.loc[:, col] = np.log1p(df_filtered[col])
# Drop missing values from important columns
df_filtered.dropna(subset=numeric_cols + ['shipment mode', 'vendor inco term', 'country', 'product group'], inplace=True)

# Add time-based features
df_filtered['month'] = df_filtered['po sent to vendor date'].dt.month
df_filtered['year'] = df_filtered['po sent to vendor date'].dt.year
df_filtered['quarter'] = df_filtered['po sent to vendor date'].dt.quarter

import plotly.express as px

# CHOROPLETHS

# Total Shipment Freight Cost (USD) by Country
# Group and sum by country
shipment_by_country = df_filtered.groupby('country')['freight cost (usd)'].sum().reset_index()
# Plot
fig_map = px.choropleth(
    shipment_by_country,
    locations='country',
    locationmode='country names',
    color='freight cost (usd)',
    color_continuous_scale='Blues',
    title='Total Shipment Freight Cost (USD) by Country')
fig_map.show()

# Total Shipment Freight Weight (Kilograms) by Country
# Replace with the appropriate column if needed
df_filtered['weight (kilograms)'] = pd.to_numeric(df_filtered['weight (kilograms)'], errors='coerce')
# Group and sum by country
shipment_by_country = df_filtered.groupby('country')['weight (kilograms)'].sum().reset_index()
# Plot
fig_map = px.choropleth(
    shipment_by_country,
    locations='country',
    locationmode='country names',
    color='weight (kilograms)',
    color_continuous_scale='Blues',
    title='Total Shipment Weight (Volume) by Country')
fig_map.show()

# PAIRWISE

# Pairwise Feature Relationships
sns.pairplot(df_filtered[numeric_cols])  # Choose top features
plt.suptitle("Pairwise Feature Relationships", y=1.02)
plt.show()

#cost, weight, and lead time:
fig = px.scatter_3d(df_filtered, x='weight (kilograms)', y='lead_time', z='freight cost (usd)',
                    color='shipment mode', title='3D View of Shipment Features')
fig.show()

import pandas as pd
import plotly.express as px
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# PCA FOR FREQUENCY OF SHIPMENT TO A COUNTRY

# Count how many times each country appears
country_counts = df_filtered['country'].value_counts()
# Get thresholds
high_threshold = np.percentile(country_counts.values, 66)
low_threshold = np.percentile(country_counts.values, 33)
# Define a function to group countries
def group_country(country):
    count = country_counts.get(country, 0)
    if count >= high_threshold:
        return 'High Volume'
    elif count >= low_threshold:
        return 'Medium Volume'
    else:
        return 'Low Volume'
# Apply the group to the dataframe
df_filtered['country_group'] = df_filtered['country'].apply(group_country)

# Prepare features
numeric_features = ['freight cost (usd)', 'weight (kilograms)', 'lead_time']
df_pca = df_filtered.dropna(subset=numeric_features + ['country_group'])

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_pca[numeric_features])

# Perform PCA
pca = PCA(n_components=2)
components = pca.fit_transform(X_scaled)
df_pca['PC1'] = components[:, 0]
df_pca['PC2'] = components[:, 1]

# Visualize, color by grouped country
fig = px.scatter(
    df_pca,
    x = 'PC1',
    y = 'PC2',
    color = 'country_group',
    title = 'PCA of Shipment Features Colored by Top 3 Country Groups',
    labels = {'country_grouped': 'Country Group'},
    template = 'plotly_white')
fig.update_layout(title_font_size=20)
fig.show()

"""*What are the most important features that influence lead time?*

> Gradient Boosting Regressor

"""

# GRADIENT BOOSTING REGRESSOR

# ATTEMPT 1

import pandas as pd
import numpy as np
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import matplotlib.pyplot as plt
import plotly.express as px
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.svm import SVR
import seaborn as sns

# Feature Engineering
features = ['country', 'shipment mode', 'vendor inco term', 'product group', 'freight cost (usd)', 'weight (kilograms)', 'month', 'year', 'quarter']
X = df_filtered[features]
y = df_filtered['lead_time']
# Categorical columns
categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
# Preprocessing
preprocessor = ColumnTransformer(transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),
        ('num', Pipeline(steps=[
            ('skew', PowerTransformer(method='yeo-johnson', standardize=False)),
            ('scale', StandardScaler())
        ]), numeric_cols)])
#preprocessor = ColumnTransformer(transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)], remainder='passthrough')

# Regressors: Gradient Boosting, Random Forest, SVR
models = {
    'GradientBoosting': GradientBoostingRegressor(random_state=42),
    'RandomForest': RandomForestRegressor(random_state=42),
    'SVR': SVR()}
#model = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', GradientBoostingRegressor(random_state=42))])
results = {}
for name, regressor in models.items():
    pipe = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', regressor)])

# Cross-validation
scores = cross_val_score(pipe, X, y, cv=5, scoring='r2')
results[name] = {
    'mean_r2': np.mean(scores),
    'std_r2': np.std(scores)}
print(f"{name} R²: {scores.mean():.3f} ± {scores.std():.3f}")

# Hyperparameter tuning
param_grid = {
    'regressor__n_estimators': [100, 200],
    'regressor__max_depth': [3, 5, 7],
    'regressor__learning_rate': [0.01, 0.1]}
best_pipe = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', GradientBoostingRegressor(random_state=42))])
grid = GridSearchCV(best_pipe, param_grid, cv=5, scoring='r2', n_jobs=-1)
grid.fit(X, y)
print("Best Parameterss:", grid.best_params_)
print("Best CV R² Score:", grid.best_score_)

#Best Parameters: {'regressor__learning_rate': 0.1, 'regressor__max_depth': 7, 'regressor__n_estimators': 100}
#Best CV R² Score: 0.23791485882337776

# Train/Test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
final_model = grid.best_estimator_
final_model.fit(X_train, y_train)
# Evals
y_pred = final_model.predict(X_test)
print("Mean Absolute Error:", mean_absolute_error(y_test, y_pred))
print("Root Mean Squared Error:", np.sqrt(mean_squared_error(y_test, y_pred)))
print("R²:", r2_score(y_test, y_pred))

# Mean Absolute Error: 0.06877065144114564
# Root Mean Squared Error: 0.09269457933708254
# R²: 0.29022510588808836

from sklearn.compose import make_column_selector as selector

# After model fitting:
preprocessor = final_model.named_steps['preprocessor']
reg = final_model.named_steps['regressor']

# Get feature names from OneHotEncoder (categorical)
ohe = preprocessor.named_transformers_['cat']
ohe_feature_names = ohe.get_feature_names_out(categorical_cols)

# Get passthrough (numerical and time) features based on remainder='passthrough'
# These are the columns that were not one-hot encoded
# Assume they are in the same order as the ones passed to the pipeline
passthrough_features = [col for col in X.columns if col not in categorical_cols]
# Combine
all_features = list(ohe_feature_names) + passthrough_features
# Check for alignment before creating Series
assert len(all_features) == len(reg.feature_importances_), f"Feature name count ({len(all_features)}) does not match importances ({len(reg.feature_importances_)})"

# Visualize feature importances
importances = pd.Series(reg.feature_importances_, index=all_features)
top = importances.sort_values(ascending=False).head(10)

plt.figure(figsize=(8, 5))
sns.barplot(x=top.values, y=top.index)
plt.title("Top 10 Features That Influence Lead Time")
plt.ylabel("Feature Names")
plt.xlabel("Importance")
plt.tight_layout()
plt.show()

# ATTEMPT 2

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Define features and target
categorical_cols = ['shipment mode', 'vendor inco term', 'country']
X = df_filtered[categorical_cols + ['weight (kilograms)', 'lead_time']]
y = df_filtered['freight cost (usd)']

# Preprocessing
encoder = ColumnTransformer(transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)], remainder='passthrough')

# Gradient Boosting with Pipeline
pipeline = Pipeline([('preprocessor', encoder),('model', GradientBoostingRegressor(random_state=42))])

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Grid Search (optional for tuning)
param_grid = {
    'model__n_estimators': [100, 200],
    'model__learning_rate': [0.05, 0.1],
    'model__max_depth': [3, 5]}
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)

# Evaluation
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("Best Parameters:", grid_search.best_params_)
print(f"Mean Absolute Error: {mae:.2f}") # Ideal 0
print(f"Mean Squared Error: {mse:.2f}") #Ideal 0
print(f"Root Mean Squared Error: {rmse:.2f}") # Ideal 0
print(f"R² Score: {r2:.4f}") #Ideal 1

#Best Parameters: {'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__n_estimators': 100}
#Mean Absolute Error: 0.05
#Mean Squared Error: 0.00
#Root Mean Squared Error: 0.07
#R² Score: 0.7809

import plotly.express as px

# Extract feature names
onehot = grid_search.best_estimator_.named_steps['preprocessor'].named_transformers_['cat']
onehot_features = onehot.get_feature_names_out()

# Feature names
numeric_features = ['weight (kilograms)', 'lead_time']

# Importances from Gradient Boosting
importances = grid_search.best_estimator_.named_steps['model'].feature_importances_
feat_importance_series = pd.Series(importances, index=all_features)
# Top 10
top_features = feat_importance_series.nlargest(10).sort_values()

print(feat_importance_series)

# Most important features to this model:
# weight (kilograms) – Over 81% of the model’s predictive power comes from shipment weight.
  # This makes sense since heavier shipments generally cost more to transport.
# vendor inco term_CIP – Incoterm (Carriage and Insurance Paid) strongly influences cost.
  # Suggests that terms where the vendor pays for more services (like insurance and transport) raise the freight cost.
# vendor inco term_EXW – EXW (Ex Works) means buyer bears almost all shipping costs — variation in this term significantly affects freight cost.
# country_Vietnam – Freight costs from Vietnam seem more variable or higher on average — strong enough to influence predictions.

# Least important features to this model:
# Many countries and shipment modes have near-zero importance (eg; shipment mode_Air Charter, vendor inco term_DAP, country_Benin, country_Pakistan, etc.)
  # These values either occur infrequently or do not meaningfully change the cost.

# Visualize
fig = px.bar(
    top_features,
    orientation='h',
    labels={'value': 'Importance Score', 'index': 'Feature'},
    title='Top 10 Most Important Features in Gradient Boosting Model',
    color=top_features,
    color_continuous_scale='Viridis')
fig.update_layout(
    title_font_size=20,
    xaxis_title='Importance Score',
    yaxis_title='Feature',
    template='plotly_white',
    coloraxis_showscale=False)
fig.show()
all_features = np.concatenate([onehot_features, numeric_features])

"""*How do ARV and HIV lab commodity prices vary across countries and over time?*

> Time Series Analysis


"""

# Time-Series
# Continuous time index, single target variable (unit price), time structure (years)
# Aggregate average unit price by year and country
df_time_series = df_filtered.groupby(['year', 'country'])['unit price'].mean().reset_index()
# Pivot table to make each country a column
df_time_series = df_time_series.pivot(index='year', columns='country', values='unit price')
# Fill missing values (optional: forward-fill)
df_time_series = df_time_series.fillna(method='ffill')
# Reset index for time series modeling
df_time_series.index = pd.to_datetime(df_time_series.index, format='%Y')

from statsmodels.tsa.arima.model import ARIMA
from prophet import Prophet

# Example: Forecast for a specific country )
country = 'South Africa'
series = df_time_series[country].dropna()
# Fit ARIMA model
model = ARIMA(series, order=(1,1,1))  # (p,d,q) order needs tuning
model_fit = model.fit()
# Forecast next 3 years
forecast = model_fit.forecast(steps=3)
print(forecast)

# For seasonality and external regressors
# Prepare data for Prophet
df_prophet = df_filtered[['delivered to client date', 'unit price']].dropna()
df_prophet.rename(columns={'delivered to client date': 'ds', 'unit price': 'y'}, inplace=True)
# Fit model
model = Prophet()
model.fit(df_prophet)
# Make future predictions
future = model.make_future_dataframe(periods=36, freq='M')  # Forecast next 3 years
forecast = model.predict(future)
# Visualize
model.plot(forecast)

# Forecasting price/year (rising/lowering)
# ARIMA accounts for past trends, so if prices were increasing before, it projects a continued increase
# Trend forecasting based on historical data
# Visualization shows (rising/lowering) trend over time
# Shows seasonal price fluctuations (e.g., prices drop in Q1 every year)
# Confidence interval (range of possible future values)
# If the model detects a periodic dip (e.g., price drops every 5 years), it adjusts accordingly???
# Doesn't handle external factors like inflation or policy changes

# Melt the pivoted DataFrame for long-format plotly compatibility
df_long = df_time_series.reset_index().melt(id_vars='year', var_name='country', value_name='unit_price')
df_long['year'] = df_long['year'].dt.year
fig = px.line(
    df_long,
    x='year',
    y='unit_price',
    color='country',
    title="ARV and HIV Lab Commodity Prices Over Time by Country",
    labels={'unit_price': 'Average Unit Price (USD)', 'year': 'Year'})
fig.update_layout(legend_title_text='Country', hovermode='x unified')
fig.show()

"""*How can we optimize the supply chain expenses to reduce overall costs while maintaining efficient delivery?*

> Optimization Algorithms
"""

import pandas as pd
import numpy as np
from scipy.optimize import linprog

# Filter and clean dataset
df = shipment_pricing.copy()
# Convert dates
df['po sent to vendor date'] = pd.to_datetime(df['po sent to vendor date'], errors='coerce')
df['delivered to client date'] = pd.to_datetime(df['delivered to client date'], errors='coerce')
# Calculate lead time
df['lead_time'] = (df['delivered to client date'] - df['po sent to vendor date']).dt.days
# Clean numerical values
numeric_cols = ['freight cost (usd)', 'weight (kilograms)', 'lead_time']
df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')
# Drop NaNs and only keep air and sea (if truck is rare)
df.dropna(subset=numeric_cols, inplace=True)
df = df[df['shipment mode'].isin(['Air', 'Sea'])]

# Cost vector: freight cost per shipment
c = df['freight cost (usd)'].values

# Maximum Constraints
A = [
    # Total weight <= max
    df['weight (kilograms)'].values,
    # Total lead time <= max avg * n
    df['lead_time'].values]
b = [
    # max weight
    100000,
    # max total lead time
    25 * len(df)]

# Minimum constraints
min_weight = 50000
# min weight ≥
A.append([-w for w in df['weight (kilograms)'].values])
b.append(-min_weight)

# Bounds: [0, 1] per shipment (fractional selection)
x_bounds = [(0, 1) for _ in range(len(df))]

# Solve
result = linprog(c=c, A_ub=A, b_ub=b, bounds=x_bounds, method='highs')

if result.success:
    df['selected'] = result.x
    total_cost = result.fun
    print(f"Optimization successful. Total optimized cost: ${total_cost:,.2f}")
    print(df[df['selected'] > 0.01][['country', 'freight cost (usd)', 'lead_time', 'weight (kilograms)', 'selected']].head())
else:
    print("Optimization failed:", result.message)

# Cost vector: freight cost per shipment
c = df['freight cost (usd)'].values

# Weight constraint: total weight <= max_weight
max_weight = 100000
A_weight = [df['weight (kilograms)'].values]
b_weight = [max_weight]

# Lead time constraint: average lead time <= max_lead
max_avg_lead_time = 25
A_lead = [df['lead_time'].values]
b_lead = [max_avg_lead_time * len(df)]  # Total lead time for all shipments

# Minimum number of shipments constraint
A_min_shipments = [-np.ones(len(df))]  # Sum(x) ≥ 10 → -sum(x) ≤ -10
b_min_shipments = [-10]

# Combine constraints
A = A_weight + A_lead + A_min_shipments
b = b_weight + b_lead + b_min_shipments

# Set bounds for decision variables: each shipment is 0 (not selected) to 1 (selected fractionally)
x_bounds = [(0, 1) for _ in range(len(df))]

# Solve linear program
result = linprog(c=c, A_ub=A, b_ub=b, bounds=x_bounds, method='highs')

if result.success:
    df['selected'] = result.x
    total_cost = result.fun
    print(f"Optimization successful. Total optimized cost: ${total_cost:,.2f}")
    print(df[['country', 'shipment mode', 'freight cost (usd)', 'weight (kilograms)', 'lead_time', 'selected']].head(10))
else:
    print("Optimization failed:", result.message)

"""*Can we identify unusual patterns or outliers in pricing or shipment data that may indicate issues in the supply chain?*

> Gaussian Processes, K-Nearest Neighbors (KNN)
"""

# GAUSSIAN PROCESS

import pandas as pd
import numpy as np
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

# ATTEMPT 1

# Define input features and target
features = ['country', 'shipment mode', 'vendor inco term', 'weight (kilograms)', 'freight cost (usd)', 'product group']
target = 'lead_time'
X = df_filtered[features]
y = df_filtered[target]

# Categorical and numerical separation
categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
numerical_cols = [col for col in X.columns if col not in categorical_cols]

# Preprocessing pipeline
preprocessor = ColumnTransformer(transformers=[('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols),('num', StandardScaler(), numerical_cols)])

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define Gaussian Process with RBF kernel
kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))
gpr = GaussianProcessRegressor(kernel=kernel, alpha=1e-2, normalize_y=True)
# Pipeline
pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', gpr)])

# Fit model
pipeline.fit(X_train, y_train)
# Predict on test data with standard deviation
y_pred, y_std = pipeline.named_steps['regressor'].predict(pipeline.named_steps['preprocessor'].transform(X_test), return_std=True)

# Compute residuals
residuals = y_test - y_pred
z_scores = residuals / y_std

# Identify outliers: where |z| > threshold
threshold = 2.5
outliers = np.abs(z_scores) > threshold

# Add output for analysis
results = X_test.copy()
results['actual'] = y_test
results['predicted'] = y_pred
results['std'] = y_std
results['z_score'] = z_scores
results['outlier'] = outliers

# Visualize residuals
plt.figure(figsize=(10, 6))
sns.histplot(z_scores, bins=30, kde=True)
plt.axvline(threshold, color='red', linestyle='--', label='Outlier Threshold')
plt.axvline(-threshold, color='red', linestyle='--')
plt.title('Distribution of Z-scores from GPR Residuals')
plt.xlabel('Z-score')
plt.legend()
plt.tight_layout()
plt.show()
# Show top outliers
print("Top potential outliers based on GPR:")
print(results[results['outlier']].sort_values(by='z_score', key=np.abs, ascending=False).head(10))

# Extremely low standard deviations = overconfidence
  # Some predictions have repeated uncertainly values/standard deviations
# Unrealistically large z-scores

from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel as C
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd

# --- Preprocessing ---

categorical_cols = ['country', 'shipment mode', 'vendor inco term', 'product group']
numeric_cols = ['weight (kilograms)', 'freight cost (usd)']

X = df_filtered[categorical_cols + numeric_cols]
y = df_filtered['lead_time']

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define preprocessing
preprocessor = ColumnTransformer([
    ('cat', OneHotEncoder(handle_unknown='ignore', sparse=False), categorical_cols),
    ('num', StandardScaler(), numeric_cols)
])

# Define kernel with sensible bounds
kernel = C(1.0, (1e-2, 1e2)) * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10.0)) + WhiteKernel(noise_level=1.0, noise_level_bounds=(1e-3, 1e1))

# Define GPR model
gpr = GaussianProcessRegressor(kernel=kernel, alpha=1e-4, normalize_y=True, random_state=42)

# Create pipeline
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', gpr)
])

# --- Fit model ---
pipeline.fit(X_train, y_train)

# --- Predict with std dev for uncertainty ---
X_test_transformed = pipeline.named_steps['preprocessor'].transform(X_test)
y_pred, y_std = pipeline.named_steps['regressor'].predict(X_test_transformed, return_std=True)

# --- Flag outliers based on z-score ---
z_scores = (y_test.values - y_pred) / y_std
outliers = np.abs(z_scores) > 2  # You can also try 2.5 or 3

# --- Create result DataFrame ---
results = X_test.copy()
results['actual'] = y_test.values
results['predicted'] = y_pred
results['std'] = y_std
results['z_score'] = z_scores
results['outlier'] = outliers

# Sort by absolute z-score (strongest outliers first)
top_outliers = results.loc[results['outlier']].sort_values(by='z_score', key=np.abs, ascending=False)

print("Top potential outliers based on improved GPR:")
print(top_outliers.head(10))

# ATTEMPT 2

# Define input features and target
features = ['country', 'shipment mode', 'vendor inco term', 'weight (kilograms)', 'freight cost (usd)', 'product group']
target = 'lead_time'
X = df_filtered[features]
y = df_filtered[target]

# Categorical and numerical separation
categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
numerical_cols = [col for col in X.columns if col not in categorical_cols]

preprocessor = ColumnTransformer(transformers=[('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols), ('num', StandardScaler(), numerical_cols)])

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define Gaussian Process with RBF kernel
# Smaller lower bound
kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-5, 1e5))
regressor = GaussianProcessRegressor(kernel=kernel, normalize_y=True, random_state=42)
gpr = GaussianProcessRegressor(kernel=kernel, alpha=1e-2, normalize_y=True)
# Pipeline
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', gpr)])

# Fit model
pipeline.fit(X_train, y_train)
# Predict on test data with standard deviation
y_pred, y_std = pipeline.named_steps['regressor'].predict(
    pipeline.named_steps['preprocessor'].transform(X_test), return_std=True)

# Compute residuals
residuals = y_test - y_pred
z_scores = residuals / y_std

# Identify outliers: where |z| > threshold
threshold = 2.5
outliers = np.abs(z_scores) > threshold

# Add output for analysis
results = X_test.copy()
results['actual'] = y_test
results['predicted'] = y_pred
results['std'] = y_std
results['z_score'] = z_scores
results['outlier'] = outliers

# Visualize residuals
plt.figure(figsize=(10, 6))
sns.histplot(z_scores, bins=30, kde=True)
plt.axvline(threshold, color='red', linestyle='--', label='Outlier Threshold')
plt.axvline(-threshold, color='red', linestyle='--')
plt.title('Distribution of Z-scores from GPR Residuals')
plt.xlabel('Z-score')
plt.legend()
plt.tight_layout()
plt.show()
# Show top outliers
print("Top potential outliers based on GPR:")
print(results[results['outlier']].sort_values(by='z_score', key=np.abs, ascending=False).head(10))

# Identical predictions and standard deviations = overfitting
# Standard deviations too small / similar
# When different rows yield the same output, the model is likely not differentiating features well, which could stem from:
  # Over-simplified encoding, feature collinearity, pipeline errors, or an inappropriate kernel setup that “flattens” the response surface.
# Z-score thresholds catching everything

# K-NEAREST NEIGHBORS

from sklearn.neighbors import NearestNeighbors

# Numeric features for anomaly detection
features_for_knn = ['freight cost (usd)', 'weight (kilograms)', 'lead_time']
data_knn = df_filtered[features_for_knn].copy()

# Standardize the features
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data_knn)

# Fit KNN model
k = 5
knn = NearestNeighbors(n_neighbors=k)
knn.fit(data_scaled)

# Calculate distances to k (5) nearest neighbors
distances, _ = knn.kneighbors(data_scaled)

# Use the mean distance to k-nearest neighbors as anomaly score
knn_scores = distances.mean(axis=1)

# Set threshold (e.g., 95th percentile of distances)
threshold = np.percentile(knn_scores, 95)

# Add to DataFrame
df_filtered['knn_anomaly_score'] = knn_scores
df_filtered['is_outlier_knn'] = df_filtered['knn_anomaly_score'] > threshold

# Potential outliers
outliers = df_filtered[df_filtered['is_outlier_knn']].sort_values(by='knn_anomaly_score', ascending=False)
print("Top potential outliers using KNN distance method:")
print(outliers[features_for_knn + ['knn_anomaly_score']].head(10))

# Pricing, weight, or lead time irregularities that deviate from the typical supply chain pattern.
# Each deviation is a shipment identifiable by ID (2-4 digit number on left)
# 35: Extreme deviation from local neighborhood, likely due to very low freight cost (0.44 USD) for a normal weight. Perhaps a data entry error or an underpriced shipment.
# 3004, 126, 3286: Moderate deviation from local neighborhood, ikely low weight or atypical combinations of weight, freight, and lead time.
# 653, 737, 5057, 689, 547, 4923: Likley represent uncommon but valid shipments, or inconsistencies that deserve attention (eg, relatively high freight for low weights).

# Visualize KNN
fig = px.scatter(
    df_filtered,
    x='freight cost (usd)',
    y='weight (kilograms)',
    color='is_outlier_knn',
    color_discrete_map={False: 'blue', True: 'red'},
    hover_data=['lead_time', 'knn_anomaly_score'],
    title='KNN Outlier Detection: Freight Cost (USD) vs Weight (Kilograms)',
    labels={
        'freight cost (usd)': 'Freight Cost (USD)',
        'weight (kilograms)': 'Weight (kg)',
        'is_outlier_knn': 'Outlier'},
    template='plotly_white')
fig.update_traces(marker=dict(size=6, opacity=0.7))
fig.update_layout(title_font_size=20)
fig.show()